\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{CambridgeUS}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{tikz}
\usepackage{amsmath}
\usetikzlibrary{trees}

% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=3cm, sibling distance=2.5cm]
\tikzstyle{level 2}=[level distance=3.5cm, sibling distance=1.2cm]
\tikzstyle{level 3}=[level distance=3cm, sibling distance=0.6cm]

% Define styles for bags and leafs
\tikzstyle{bag} = [text width=2em, text centered]
\tikzstyle{end} = [circle, minimum width=3pt,fill, inner sep=0pt]

\newtheorem{prop}{Proposition}

\title{ENGG2430D Tutorial 6}
\author{Zhibo Yang}
\institute{\textit{Department of Information Engineering \\ The Chinese University of Hong Kong}}
\date{\textit{March 10, 2015}}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

% automatically generated outline.
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

\section{Expectation of the Sum of Random Variables}
\subsection{Linearity of Expecation}
\begin{frame}{Linearity of Expecation}
We learned the following proposition from the class:
\begin{prop}
Let $X_1$, $X_2$, $\cdots$, $X_n$ be random variables, we have
$$E[X_1+X_2+\cdots+X_n]=E[X_1]+E[X_2]+\cdots+E[X_n]$$
\end{prop}
\vspace{0.3cm}
\uncover<2->{Actually, we can be more ambitious:
\begin{block}{Linearity of Expectation}
Let $X_1$, $X_2$, $\cdots$, $X_n$ be random variables, we have
$$E[\alpha_1X_1+\alpha_2X_2+\cdots+\alpha_nX_n]=\alpha_1E[X_1]+\alpha_2E[X_2]+\cdots+\alpha_nE[X_n]$$
\end{block}
Note that $X_1$, $X_2$, $\cdots$, $X_n$ do not have to be independent!\\}
\end{frame}

\subsection{Proof}
\begin{frame}{Proof:}
  For simplicity, we let $n=2$.\\
  \uncover<2->{\begin{align*}
  E[\alpha X_1 + \beta X_2] &= \sum_{x_1}\sum_{x_2}(\alpha x_1+\beta x_2)Pr(X_1=x_1,X_2=x_2)\\
  &=\alpha\sum_{x_1}x_1\sum_{x_2}Pr(X_1=x_1,X_2=x_2)+\\
  &\,\,\,\,\,\,\,\beta\sum_{x_2}x_2\sum_{x_1}Pr(X_1=x_1,X_2=x_2)\\
  &=\alpha\sum_{x_1}x_1Pr(X_1=x_1)+\beta\sum_{x_2}x_2Pr(X_2=x_2)\\
  &=\alpha E[X_1]+\beta E[X_2]
  \end{align*}}
  \uncover<3->{The proof is similar for $n>2$.}
\end{frame}

\subsection{Example}
\begin{frame}{Calculate Expectation}
  \begin{block}{}
  \textbf{Example } Calculate the expectation of a binomial random variable having parameters $n$ and $p$ without using the its PMF.
  \end{block}
  \uncover<2->{
  Recalling that such a random variable $X$ represents the number of successes in $n$ trials when each trial has probability $p$ of being a success, we have $$X=X_1+X_2+\cdots+X_n$$
  where $X_i$ are Bernoulli variables, i.e.,
    \[ X_i = 
    \left\{ \begin{array}{l}
     1, \text{ \ \ \ \ \ \ \ \ if the } i \text{th trial is a success} \\
     0, \text{ \ \ \ \ \ \ \ \ if the } i \text{th trial is a failure} 
    \end{array} \right.
  \]
  whose expectation is $p$. Thus,
  $$E[X]=E[X_1]+E[X_2]+\cdots+E[X_n]=np$$}
\end{frame}

\section{Variance of the Sum of Random Variables}
\subsection{Definition of Covariance}
\begin{frame}{Covariance}
\uncover<2->{
The covariance of any two random variables $X$ and $Y$, denoted by $Cov(X, Y)$, is defined by
$$Cov(X,Y)=E[(X-E[X])(Y-E[Y])]$$}
\uncover<3->{
\textbf{Remarks:} \begin{enumerate}
\item Expanding the equation above, we have
\begin{align*}
Cov(X,Y)&=E[(X-E[X])(Y-E[Y])]\\
&=E[XY-YE[X]-XE[Y]+E[X]E[Y]]\\
&=E[XY]-E[Y]E[X]-E[X]E[Y]+E[X]E[Y]\\
&=E[XY]-E[X]E[Y]
\end{align*}}
\vspace{-0.5cm}
\uncover<4->{\item Covariance is a measure of how much two random variables change together.}
\uncover<5->{\item Covariance characterizes the degree of the dependence between two random variables.}
\end{enumerate}
\end{frame}

\subsection{Independence and Covariance}
\begin{frame}{Independence and Covariance}
\vspace{-0.5cm}
$$Cov(X,Y)=E[XY]-E[X]E[Y]$$
\uncover<2->{
Let us consider a simpler case where $X$ and $Y$ are indicator variables for whether the events $A$ and $B$ occur, i.e., }
\uncover<3->{
\begin{eqnarray*}
  X_{} = \left\{ \begin{array}{l}
    1, \text{ \ \ \ \ \ \ if } A \text{ occurs}\\
    0, \text{ \ \ \ \ \ \ otherwise} 
  \end{array} \right. &  & Y_{} = \left\{ \begin{array}{l}
    1, \text{ \ \ \ \ \ \ if } B \text{ occurs}\\
    0, \text{ \ \ \ \ \ \ otherwise} 
  \end{array} \right.
\end{eqnarray*}
Thus we have $Cov(X,Y)=P\{X=1,Y=1\}-P\{X=1\}P\{Y=1\}$.}
\begin{enumerate}
\uncover<4->{\item $Cov(X,Y)=0$ is equivalent to $X$ and $Y$ are independent.}
\uncover<5->{\item $Cov(X,Y)>0$ indicates that the outcome that $X=1$ makes it more likely that $Y=1$.
\vspace{-0.1cm}
\begin{align*}
Cov(X,Y)>0 &\Leftrightarrow P\{X=1,Y=1\}>P\{X=1\}P\{Y=1\}\\
&\Leftrightarrow \frac{P\{X=1,Y=1\}}{P\{Y=1\}} > P\{X=1\}\\
&\Leftrightarrow P\{Y=1\mid X=1\} > P\{X=1\}
\end{align*}}
\vspace{-0.5cm}
\uncover<6->{\item If $Cov(X,Y)<0$, then it's the other way around.}
\end{enumerate}
\end{frame}

\subsection{Properties of Covariance}
\begin{frame}{Some Properties of Covariance}
\vspace{-0.4cm}
\uncover<2->{
\begin{block}{Properties}
\begin{enumerate}
\item $Cov(X,X)=Var(X)$,
\item $Cov(X,Y)=Cov(Y,X)$
\item $Cov(cX,Y)=c Cov(X,Y)$
\item $Cov(X,Y+Z)=Cov(X,Y)+Cov(X,Z)$
\end{enumerate}
\end{block}}
\uncover<3->{
\textbf{Proof of 4:} 
\vspace{-0.2cm}
\begin{align*}
Cov(X,Y+Z)&=E[X(Y+Z)]-E[X]E[Y+Z]\\
&=E[XY]+E[XZ]-E[X]E[Y]-E[X]E[Z]\\
&=(E[XY]-E[X]E[Y])+(E[XZ]-E[X]E[Z])\\
&=Cov(X,Y)+Cov(X,Z)
\end{align*}}
\vspace{-0.3cm}
\uncover<4->{
\hspace{-0.25cm}\textbf{Corollary of 4:} 
$$Cov\Big(\sum_{i=1}^nX_i,\sum_{j=1}^mY_j\Big)=\sum_{i=1}^n\sum_{j=1}^mCov(X_i,Y_j)$$}
\end{frame}

\subsection{Variance of Sum of Random Variables}
\begin{frame}{Variance of Sum of Random Variables}
A useful expression for the variance of the sum of random variables can be
obtained from Corollary of 4 as follows: 
\uncover<2->{
\begin{align*}
Var\Big(\sum_{i=1}^nX_i\Big)&=Cov\Big(\sum_{i=1}^nX_i,\sum_{j=1}^nX_j\Big)\\
&=\sum_{i=1}^n\sum_{j=1}^nCov(X_i,X_j)\\
&=\sum_{i=1}^nCov(X_i,X_i)+\sum_{i=1}^n\sum_{j\neq i}Cov(X_i,X_j)\\
&=\sum_{i=1}^nVar(X_i)+2\sum_{i=1}^n\sum_{j<i}Cov(X_i,X_j)
\end{align*}}
\end{frame}

\subsection{Example}
\begin{frame}{Variance of Sum of Random Variables (cont'd)}
If $X_i,i=1,\dots,n$ are independent with each other, then we have $Cov(X_i,X_j)=0$ where $i\neq j$, thus $$Var\Big(\sum_{i=1}^nX_i\Big)=\sum_{i=1}^nVar(X_i)$$
\vspace{-0.3cm}
\uncover<2->{
\begin{block}{}
  \textbf{Example } Calculate the variance of a binomial random variable having parameters $n$ and $p$ without using the its PMF.
\end{block}}
\uncover<3->{
  Setting are the same as before, i.e., $X=X_1+X_2+\cdots+X_n$ and
    \[ X_i = 
    \left\{ \begin{array}{l}
     1, \text{ \ \ \ \ \ \ \ \ if the } i \text{th trial is a success} \\
     0, \text{ \ \ \ \ \ \ \ \ if the } i \text{th trial is a failure} 
    \end{array} \right.
  \]
  whose variance is $Var(X_i)=E[X^2]-E[X]^2=E[X]-E[X]^2=p-p^2$. thus,
  $$Var(X)=\sum_{i=1}^nVar(X_i)=np(1-p)$$}
\end{frame}

\end{document}
